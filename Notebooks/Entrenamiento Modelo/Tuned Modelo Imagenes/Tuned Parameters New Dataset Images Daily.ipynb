{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "sys.path.append('..')\n",
    "import CustomHyperModelImages\n",
    "import EnergyPricesLibrary as Ep\n",
    "\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,scaler_y,trainX,trainY,testX,testY,n_steps_out,len_output_features):\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    trainPredict = trainPredict.reshape(trainPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    testPredict  = model.predict(testX)\n",
    "    testPredict  = testPredict.reshape(testPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY_ = scaler_y.inverse_transform(trainY.reshape(trainY.shape[0]*n_steps_out,len_output_features))\n",
    "    \n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY_ = scaler_y.inverse_transform(testY.reshape(testY.shape[0]*n_steps_out,len_output_features))\n",
    "        \n",
    "    return trainPredict,trainY_,testPredict,testY_\n",
    "\n",
    "def get_metrics(trainY,trainPredict,testY,testPredict):\n",
    "    \n",
    "    trainMAPE  = Ep.MAPE(trainPredict,trainY)\n",
    "    testMAPE  = Ep.MAPE(testPredict,testY)\n",
    "    \n",
    "    train_sMAPE  = Ep.sMAPE(trainY,trainPredict)\n",
    "    test_sMAPE  = Ep.sMAPE(testY,testPredict)\n",
    "    \n",
    "    return trainMAPE,testMAPE,train_sMAPE,test_sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ns3_resource = boto3.resource('s3',\\n                             aws_access_key_id='AKIA4NVVYWBFHY2KRSMC',\\n                             aws_secret_access_key='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')\\nbucket = s3_resource.Bucket('colombia-energy-forecast')\\n\\nfor obj in bucket.objects.filter():\\n    if not os.path.exists(os.path.dirname(obj.key)):\\n        os.makedirs(os.path.dirname(obj.key))\\n    if '.xlsx' in obj.key or '.jpg' in obj.key:\\n        bucket.download_file(obj.key, obj.key) # save to same path\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "s3_resource = boto3.resource('s3',\n",
    "                             aws_access_key_id='AKIA4NVVYWBFHY2KRSMC',\n",
    "                             aws_secret_access_key='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')\n",
    "bucket = s3_resource.Bucket('colombia-energy-forecast')\n",
    "\n",
    "for obj in bucket.objects.filter():\n",
    "    if not os.path.exists(os.path.dirname(obj.key)):\n",
    "        os.makedirs(os.path.dirname(obj.key))\n",
    "    if '.xlsx' in obj.key or '.jpg' in obj.key:\n",
    "        bucket.download_file(obj.key, obj.key) # save to same path\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatic_images_prcp_dir = os.path.join('..','..','..','dataset','Climatic Images','PRCP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatic_images_tavg_dir = os.path.join('..','..','..','dataset','Climatic Images','TAVG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_bolsa_path = os.path.join('..','..','..','dataset','Series','Sabanas','Original','Sabana_Datos_Precio_Bolsa.xlsx')\n",
    "precio_bolsa = pd.read_excel(precio_bolsa_path)\n",
    "precio_bolsa = precio_bolsa.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_fechas = list()\n",
    "lista_rutas = list()\n",
    "for prcp_file,tavg_file in zip(os.listdir(climatic_images_prcp_dir),os.listdir(climatic_images_tavg_dir)):\n",
    "    fecha = prcp_file.split('.')[0]\n",
    "    ruta_prcp = os.path.join(climatic_images_prcp_dir,prcp_file)\n",
    "    ruta_tavg = os.path.join(climatic_images_tavg_dir,tavg_file)\n",
    "    lista_fechas.append(fecha)\n",
    "    lista_rutas.append([ruta_prcp,ruta_tavg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'All'\n",
    "start_date_train = '2000-02-01'\n",
    "start_date_val = '2020-01-01'\n",
    "start_date_test = '2020-04-01'\n",
    "end_date_test = '2020-05-01'\n",
    "n_steps_out=24\n",
    "output_columns = ['$kWh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(lista_rutas,index=lista_fechas,columns=['Precipitacion','Temperatura'])\n",
    " \n",
    "n_steps_in  = 3\n",
    "overlap = 1\n",
    "len_output_features = len(output_columns)\n",
    "\n",
    "IMG_HEIGHT,IMG_WIDTH = 128,128\n",
    "\n",
    "results = Ep.SplitTimeseriesMultipleTimesBackAhead_DifferentTimes_Images(df_x=dataset_df,df_y=precio_bolsa,\n",
    "                                                                         start_date_train=start_date_train,\n",
    "                                                                         start_date_val=start_date_val,\n",
    "                                                                         start_date_test=start_date_test,\n",
    "                                                                         end_date_test=end_date_test,n_steps_out=n_steps_out,\n",
    "                                                                         n_steps_in=n_steps_in,overlap=overlap,\n",
    "                                                                         output_features=output_columns,\n",
    "                                                                         IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "trainX_I,trainY_I,valX_I,valY_I,testX_I,testY_I,scaler_y_I,dataset_x_I,dataset_y_I = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Train:',trainX_I.shape, trainY_I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Val:',valX_I.shape,valY_I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Test:',testX_I.shape, testY_I.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                          factor=0.1,\n",
    "                                                          min_lr=1e-4,\n",
    "                                                          patience=5,\n",
    "                                                          verbose=1)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=10,\n",
    "                                                  mode='min')\n",
    "\n",
    "callbacks = [callback_reduce_lr,early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = trainX_I[0].shape\n",
    "\n",
    "arquitectura1 = CustomHyperModelImages.ArquitecturaI1(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura2 = CustomHyperModelImages.ArquitecturaI2(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura3 = CustomHyperModelImages.ArquitecturaI3(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura4 = CustomHyperModelImages.ArquitecturaI4(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura5 = CustomHyperModelImages.ArquitecturaI5(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura6 = CustomHyperModelImages.ArquitecturaI6(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura7 = CustomHyperModelImages.ArquitecturaI7(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura8 = CustomHyperModelImages.ArquitecturaI8(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura9 = CustomHyperModelImages.ArquitecturaI9(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura10 = CustomHyperModelImages.ArquitecturaI10(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura11 = CustomHyperModelImages.ArquitecturaI11(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura12 = CustomHyperModelImages.ArquitecturaI12(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arq_list = [arquitectura1,arquitectura2,arquitectura3,arquitectura4,\n",
    "            arquitectura5,arquitectura6,arquitectura7,arquitectura8,\n",
    "            arquitectura9,arquitectura10,arquitectura11,arquitectura12]\n",
    "\"\"\"\n",
    "arq_list = [arquitectura1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_idx = 1\n",
    "arq_best_models = dict()\n",
    "\n",
    "for arq in arq_list:\n",
    "    \n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        arq,\n",
    "        objective='val_loss',\n",
    "        num_initial_points=1,\n",
    "        max_trials=10,\n",
    "        directory=os.path.normpath('C:/my_dir'),\n",
    "        project_name=str(arq_idx)\n",
    "    )\n",
    "    \n",
    "    # Overview of the task\n",
    "    bayesian_tuner.search_space_summary()\n",
    "    \n",
    "    # Performs the hyperparameter tuning\n",
    "    search_start = time.time()\n",
    "    bayesian_tuner.search(x=trainX_I,y=trainY_I,\n",
    "                      epochs=200,\n",
    "                      validation_data=(valX_I,valY_I),\n",
    "                      callbacks=callbacks)\n",
    "    search_end = time.time()\n",
    "    elapsed_time = search_end - search_start\n",
    "    \n",
    "    print('Tiempo Total Transcurrido {}'.format(elapsed_time))\n",
    "    \n",
    "    dict_key = 'Arquitectura {}'.format(arq_idx)\n",
    "\n",
    "    arq_best_models[dict_key] = dict()\n",
    "    bs_model = bayesian_tuner.oracle.get_best_trials(1)[0]\n",
    "    \n",
    "    model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    trainPredict,trainY_true,testPredict,testY_true = make_predictions(model,scaler_y_I,trainX_I,trainY_I,valX_I,valY_I,\n",
    "                                                             n_steps_out,len_output_features)\n",
    "    \n",
    "    trainMAPE,testMAPE,train_sMAPE,test_sMAPE = get_metrics(trainY_true,trainPredict,testY_true,testPredict)\n",
    "\n",
    "    arq_best_models[dict_key]['Score'] = bs_model.score\n",
    "    arq_best_models[dict_key]['Tiempo Scaneo'] = elapsed_time\n",
    "    arq_best_models[dict_key]['Mape Train'] = trainMAPE\n",
    "    arq_best_models[dict_key]['Mape Test'] = testMAPE\n",
    "    arq_best_models[dict_key]['sMape Train'] = train_sMAPE\n",
    "    arq_best_models[dict_key]['sMape Test'] = test_sMAPE\n",
    "\n",
    "    if bs_model.hyperparameters.values:\n",
    "        for hp, value in bs_model.hyperparameters.values.items():\n",
    "            arq_best_models[dict_key][hp] = value\n",
    "            \n",
    "    with open('BestModels-min-{}.json', 'w') as outfile:\n",
    "        json.dump(arq_best_models, outfile)\n",
    "    \n",
    "    arq_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BestModels.json', 'w') as outfile:\n",
    "    json.dump(arq_best_models, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBAS PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,scaler_y,train_ds,test_ds,n_steps_out,len_output_features):\n",
    "    \n",
    "    sample_x = train_ds.take(1)\n",
    "    sample_y = test_ds.take(1)\n",
    "\n",
    "    trainX, trainY = next(iter(sample_x))\n",
    "    testX, testY = next(iter(sample_y))\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    trainPredict = trainPredict.reshape(trainPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    testPredict  = model.predict(testX)\n",
    "    testPredict  = testPredict.reshape(testPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY_ = scaler_y.inverse_transform(trainY.numpy().reshape(trainY.numpy().shape[0]*n_steps_out,len_output_features))\n",
    "\n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY_ = scaler_y.inverse_transform(testY.numpy().reshape(testY.numpy().shape[0]*n_steps_out,len_output_features))\n",
    "        \n",
    "    return trainPredict,trainY_,testPredict,testY_\n",
    "\n",
    "def get_metrics(trainY,trainPredict,testY,testPredict):\n",
    "    \n",
    "    trainMAPE  = Ep.MAPE(trainPredict,trainY)\n",
    "    testMAPE  = Ep.MAPE(testPredict,testY)\n",
    "    \n",
    "    train_sMAPE  = Ep.sMAPE(trainY,trainPredict)\n",
    "    test_sMAPE  = Ep.sMAPE(testY,testPredict)\n",
    "    \n",
    "    return trainMAPE,testMAPE,train_sMAPE,test_sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_images_ds(ds,IMG_HEIGHT,IMG_WIDTH):\n",
    "    img_data_list = list()\n",
    "\n",
    "    for path in ds:\n",
    "        image_prcp = _load_images(path[0],IMG_HEIGHT,IMG_WIDTH)\n",
    "        image_tavg = _load_images(path[1],IMG_HEIGHT,IMG_WIDTH)\n",
    "\n",
    "        image_concat = np.concatenate([image_prcp,image_tavg],axis=-1)\n",
    "        img_data_list.append(image_concat)\n",
    "    \n",
    "    img_data_array = np.array(img_data_list)\n",
    "\n",
    "    return img_data_array\n",
    "\n",
    "def _load_images(path,IMG_HEIGHT,IMG_WIDTH):\n",
    "    image = np.array(Image.open(path))\n",
    "    image = np.resize(image,(IMG_HEIGHT,IMG_WIDTH,3))\n",
    "    image = image.astype('float32')\n",
    "    image /= 255\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasetMultipleTimesBackAhead_differentTimes_inverse(ds_x, ds_y, n_steps_out=1, n_steps_in = 1, overlap = 1):\n",
    "    dataX, dataY = [], []\n",
    "    tem = n_steps_in + (n_steps_out/24) - overlap\n",
    "    range_iter = int((len(ds_x) - tem)/overlap)\n",
    "\n",
    "    for i in range(range_iter,0,-1):\n",
    "        startx = (i*overlap)-(overlap*int(n_steps_out/24))\n",
    "        endx = startx + n_steps_in\n",
    "        starty = endx*n_steps_out\n",
    "        endy =  starty+n_steps_out\n",
    "        \n",
    "        trainX, trainY = ds_x[startx:endx, :], ds_y[starty:endy, -1].reshape(n_steps_out,1)\n",
    "        trainX, trainY = trainX[::-1],trainY[::-1]\n",
    "        \n",
    "        yield trainX, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitTimeseriesMultipleTimesBackAhead_DifferentTimes_Images(df_x,df_y,start_date_train='2000-02-01',start_date_val='2020-01-01',\n",
    "                                                         start_date_test='2020-04-01',end_date_test='2020-05-01',n_steps_out=1,\n",
    "                                                         n_steps_in = 1, overlap = 1, output_features=None,IMG_HEIGHT=635,IMG_WIDTH=460):\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_x   = df_x.loc[(df_x.index >=  start_date_train) & (df_x.index <  start_date_val)].values\n",
    "    val_x     = df_x.loc[(df_x.index >=  start_date_val)   & (df_x.index < start_date_test)].values\n",
    "    test_x    = df_x.loc[(df_x.index >=  start_date_test)  & (df_x.index < end_date_test)].values\n",
    "\n",
    "    train_y   = df_y.loc[(df_y.index >=  start_date_train) & (df_y.index <  start_date_val)].values\n",
    "    val_y     = df_y.loc[(df_y.index >=  start_date_val)   & (df_y.index < start_date_test)].values\n",
    "    test_y    = df_y.loc[(df_y.index >=  start_date_test)  & (df_y.index < end_date_test)].values\n",
    "\n",
    "    dataset_x = np.concatenate([train_x,val_x,test_x],axis=0)\n",
    "    dataset_y = np.concatenate([train_y,val_y,test_y],axis=0)\n",
    "\n",
    "    # normalize the dataset\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_y  = scaler_y.fit_transform(train_y)\n",
    "    val_y    = scaler_y.transform(val_y)\n",
    "    test_y   = scaler_y.transform(test_y)\n",
    "\n",
    "    img_data_array = _build_images_ds(train_x,IMG_HEIGHT,IMG_WIDTH)\n",
    "    train_gen = create_datasetMultipleTimesBackAhead_differentTimes_inverse(img_data_array, \n",
    "                                                                            train_y, \n",
    "                                                                            n_steps_out=n_steps_out,\n",
    "                                                                            n_steps_in=n_steps_in, \n",
    "                                                                            overlap=overlap)\n",
    "\n",
    "    \n",
    "    val2_x = np.concatenate((train_x[-n_steps_in:].reshape(n_steps_in,-1),val_x),axis=0)\n",
    "    val2_y = np.concatenate((train_y[-(n_steps_in*24):].reshape((n_steps_in*24),-1),val_y),axis=0)\n",
    "    img_data_array = _build_images_ds(val2_x,IMG_HEIGHT,IMG_WIDTH)\n",
    "    val_gen = create_datasetMultipleTimesBackAhead_differentTimes_inverse(img_data_array,\n",
    "                                                                          val2_y,\n",
    "                                                                          n_steps_out=n_steps_out,\n",
    "                                                                          n_steps_in=n_steps_in,\n",
    "                                                                          overlap=overlap)\n",
    "    \n",
    "    test2_x = np.concatenate((val_x[-n_steps_in:].reshape(n_steps_in,-1),test_x),axis=0)\n",
    "    test2_y = np.concatenate((val_y[-(n_steps_in*24):].reshape((n_steps_in*24),-1),test_y),axis=0)\n",
    "    img_data_array = _build_images_ds(test2_x,IMG_HEIGHT,IMG_WIDTH)\n",
    "    test_gen = create_datasetMultipleTimesBackAhead_differentTimes_inverse(img_data_array,\n",
    "                                                                           test2_y,\n",
    "                                                                           n_steps_out=n_steps_out,\n",
    "                                                                           n_steps_in=n_steps_in,\n",
    "                                                                           overlap=overlap)\n",
    "    \n",
    "    return train_gen,val_gen,test_gen, scaler_y, dataset_x, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(lista_rutas,index=lista_fechas,columns=['Precipitacion','Temperatura'])\n",
    " \n",
    "n_steps_in  = 15\n",
    "overlap = 1\n",
    "len_output_features = len(output_columns)\n",
    "\n",
    "IMG_HEIGHT,IMG_WIDTH = 128,128\n",
    "\n",
    "results = SplitTimeseriesMultipleTimesBackAhead_DifferentTimes_Images(df_x=dataset_df,df_y=precio_bolsa,\n",
    "                                                                         start_date_train=start_date_train,\n",
    "                                                                         start_date_val=start_date_val,\n",
    "                                                                         start_date_test=start_date_test,\n",
    "                                                                         end_date_test=end_date_test,n_steps_out=n_steps_out,\n",
    "                                                                         n_steps_in=n_steps_in,overlap=overlap,\n",
    "                                                                         output_features=output_columns,\n",
    "                                                                         IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "\n",
    "\n",
    "train_gen,val_gen,test_gen, scaler_y_I, dataset_x, dataset_y = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds  = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes = ((n_steps_in,IMG_HEIGHT,IMG_WIDTH,6), \n",
    "                                                            (n_steps_out,len_output_features)))\n",
    "\n",
    "train_ds = train_ds.batch(batch_size=16,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds  = tf.data.Dataset.from_generator(lambda: val_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes = ((n_steps_in,IMG_HEIGHT,IMG_WIDTH,6), \n",
    "                                                            (n_steps_out,len_output_features)))\n",
    "\n",
    "val_ds = val_ds.batch(batch_size=16,drop_remainder=False)#.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds  = tf.data.Dataset.from_generator(lambda: test_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes = ((n_steps_in,IMG_HEIGHT,IMG_WIDTH,6), \n",
    "                                                            (n_steps_out,len_output_features)))\n",
    "\n",
    "test_ds = test_ds.batch(batch_size=16,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                          factor=0.1,\n",
    "                                                          min_lr=1e-4,\n",
    "                                                          patience=5,\n",
    "                                                          verbose=1)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=10,\n",
    "                                                  mode='min')\n",
    "\n",
    "callbacks = [callback_reduce_lr,early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (n_steps_in,IMG_HEIGHT,IMG_WIDTH,6)\n",
    "\n",
    "arquitectura1 = CustomHyperModelImages.ArquitecturaI1(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura2 = CustomHyperModelImages.ArquitecturaI2(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura3 = CustomHyperModelImages.ArquitecturaI3(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura4 = CustomHyperModelImages.ArquitecturaI4(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura5 = CustomHyperModelImages.ArquitecturaI5(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura6 = CustomHyperModelImages.ArquitecturaI6(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura7 = CustomHyperModelImages.ArquitecturaI7(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura8 = CustomHyperModelImages.ArquitecturaI8(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura9 = CustomHyperModelImages.ArquitecturaI9(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura10 = CustomHyperModelImages.ArquitecturaI10(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura11 = CustomHyperModelImages.ArquitecturaI11(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura12 = CustomHyperModelImages.ArquitecturaI12(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_list = [arquitectura1,arquitectura2,arquitectura3,arquitectura4,\n",
    "            arquitectura5,arquitectura6,arquitectura7,arquitectura8,\n",
    "            arquitectura9,arquitectura10,arquitectura11,arquitectura12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project C:\\my_dir\\1\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from C:\\my_dir\\1\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 13\n",
      "convLSTM2d_filters_layer_1 (Int)\n",
      "{'default': 32, 'conditions': [], 'min_value': 8, 'max_value': 32, 'step': 32, 'sampling': None}\n",
      "convLSTM2d_kernel_layer_1 (Int)\n",
      "{'default': 3, 'conditions': [], 'min_value': 3, 'max_value': 5, 'step': 2, 'sampling': None}\n",
      "conv2d_padding_layer_1 (Choice)\n",
      "{'default': 'valid', 'conditions': [], 'values': ['valid', 'same'], 'ordered': False}\n",
      "convLSTM2d_filters_layer_3 (Int)\n",
      "{'default': 32, 'conditions': [], 'min_value': 8, 'max_value': 32, 'step': 32, 'sampling': None}\n",
      "convLSTM2d_kernel_layer_3 (Int)\n",
      "{'default': 3, 'conditions': [], 'min_value': 3, 'max_value': 5, 'step': 2, 'sampling': None}\n",
      "conv2d_padding_layer_3 (Choice)\n",
      "{'default': 'valid', 'conditions': [], 'values': ['valid', 'same'], 'ordered': False}\n",
      "convLSTM2d_filters_layer_5 (Int)\n",
      "{'default': 32, 'conditions': [], 'min_value': 8, 'max_value': 32, 'step': 32, 'sampling': None}\n",
      "convLSTM2d_kernel_layer_5 (Int)\n",
      "{'default': 3, 'conditions': [], 'min_value': 3, 'max_value': 5, 'step': 2, 'sampling': None}\n",
      "conv2d_padding_layer_5 (Choice)\n",
      "{'default': 'valid', 'conditions': [], 'values': ['valid', 'same'], 'ordered': False}\n",
      "pool2d_size_layer_6 (Int)\n",
      "{'default': 3, 'conditions': [], 'min_value': 3, 'max_value': 5, 'step': 2, 'sampling': None}\n",
      "dense_units_layer_8 (Int)\n",
      "{'default': 120, 'conditions': [], 'min_value': 24, 'max_value': 120, 'step': 24, 'sampling': None}\n",
      "dense_layer_activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Float)\n",
      "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "convLSTM2d_filt...|8                 |8                 \n",
      "convLSTM2d_kern...|5                 |3                 \n",
      "conv2d_padding_...|same              |valid             \n",
      "convLSTM2d_filt...|8                 |8                 \n",
      "convLSTM2d_kern...|3                 |3                 \n",
      "conv2d_padding_...|valid             |same              \n",
      "convLSTM2d_filt...|8                 |8                 \n",
      "convLSTM2d_kern...|3                 |3                 \n",
      "conv2d_padding_...|valid             |valid             \n",
      "pool2d_size_lay...|3                 |5                 \n",
      "dense_units_lay...|96                |72                \n",
      "dense_layer_act...|sigmoid           |relu              \n",
      "learning_rate     |0.00077414        |0.0015228         \n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2500 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2b9e6f4433c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Performs the hyperparameter tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msearch_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     bayesian_tuner.search(x=train_ds,\n\u001b[0m\u001b[0;32m     27\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\proyecto-grados\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\proyecto-grados\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'callbacks'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\proyecto-grados\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \"\"\"\n\u001b[0;32m    140\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\proyecto-grados\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\proyecto-grados\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;31m# Run validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "arq_idx = 1\n",
    "arq_best_models = dict()\n",
    "\n",
    "for arq in arq_list:\n",
    "    \n",
    "    train_ds  = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                           output_types=(tf.float32, tf.float32),\n",
    "                                           output_shapes = ((n_steps_in,IMG_HEIGHT,IMG_WIDTH,6), \n",
    "                                                            (n_steps_out,len_output_features)))\n",
    "    train_ds = train_ds.batch(batch_size=16,drop_remainder=False).repeat()\n",
    "\n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        arq,\n",
    "        objective='val_loss',\n",
    "        num_initial_points=1,\n",
    "        max_trials=10,\n",
    "        directory=os.path.normpath('C:/my_dir'),\n",
    "        project_name=str(arq_idx)\n",
    "    )\n",
    "\n",
    "    # Overview of the task\n",
    "    bayesian_tuner.search_space_summary()\n",
    "\n",
    "    # Performs the hyperparameter tuning\n",
    "    search_start = time.time()\n",
    "    bayesian_tuner.search(x=train_ds,\n",
    "                          epochs=50,\n",
    "                          steps_per_epoch=50,\n",
    "                          validation_data=val_ds,\n",
    "                          callbacks=callbacks)\n",
    "    search_end = time.time()\n",
    "    elapsed_time = search_end - search_start\n",
    "\n",
    "    print('Tiempo Total Transcurrido {}'.format(elapsed_time))\n",
    "    dict_key = 'Arquitectura {}'.format(arq_idx)\n",
    "\n",
    "    arq_best_models[dict_key] = dict()\n",
    "    bs_model = bayesian_tuner.oracle.get_best_trials(1)[0]\n",
    "    model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    arq_best_models[dict_key]['Score'] = bs_model.score\n",
    "    arq_best_models[dict_key]['Tiempo Scaneo'] = elapsed_time\n",
    "\n",
    "    trainPredict,trainY_true,testPredict,testY_true = make_predictions(model,scaler_y_I,train_ds,test_ds,\n",
    "                                                                       n_steps_out,len_output_features)\n",
    "\n",
    "    trainMAPE,testMAPE,train_sMAPE,test_sMAPE = get_metrics(trainY_true,trainPredict,testY_true,testPredict)\n",
    "\n",
    "    arq_best_models[dict_key]['Score'] = bs_model.score\n",
    "    arq_best_models[dict_key]['Tiempo Scaneo'] = elapsed_time\n",
    "    arq_best_models[dict_key]['Mape Train'] = trainMAPE\n",
    "    arq_best_models[dict_key]['Mape Test'] = testMAPE\n",
    "    arq_best_models[dict_key]['sMape Train'] = train_sMAPE\n",
    "    arq_best_models[dict_key]['sMape Test'] = test_sMAPE\n",
    "\n",
    "    if bs_model.hyperparameters.values:\n",
    "        for hp, value in bs_model.hyperparameters.values.items():\n",
    "            arq_best_models[dict_key][hp] = value\n",
    "    \n",
    "    with open('BestModels-min-{}.json'.format(str(arq_idx)), 'w') as outfile:\n",
    "        json.dump(arq_best_models, outfile)\n",
    "        \n",
    "    arq_idx +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proyecto Grados",
   "language": "python",
   "name": "proyecto-grados"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
