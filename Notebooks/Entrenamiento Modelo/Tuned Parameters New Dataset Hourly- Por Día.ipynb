{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "\n",
    "import CustomHyperModel\n",
    "import CustomMetrics\n",
    "import EnergyPricesLibrary as Ep\n",
    "\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,scaler_y,trainX,trainY,testX,testY,n_steps_out,len_output_features):\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    trainPredict = trainPredict.reshape(trainPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    testPredict  = model.predict(testX)\n",
    "    testPredict  = testPredict.reshape(testPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY_ = scaler_y.inverse_transform(trainY.reshape(trainY.shape[0]*n_steps_out,len_output_features))\n",
    "    \n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY_ = scaler_y.inverse_transform(testY.reshape(testY.shape[0]*n_steps_out,len_output_features))\n",
    "        \n",
    "    return trainPredict,trainY_,testPredict,testY_\n",
    "\n",
    "def get_metrics(trainY,trainPredict,testY,testPredict):\n",
    "    \n",
    "    trainMAPE  = Ep.MAPE(trainPredict,trainY)\n",
    "    testMAPE  = Ep.MAPE(testPredict,testY)\n",
    "    \n",
    "    train_sMAPE  = Ep.sMAPE(trainY,trainPredict)\n",
    "    test_sMAPE  = Ep.sMAPE(testY,testPredict)\n",
    "    \n",
    "    return trainMAPE,testMAPE,train_sMAPE,test_sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_hourly(n_steps_in,n_steps_out,overlap,nombre_series_horaria,output_columns,data,day,\n",
    "                       start_date_train,start_date_val,start_date_test,end_date_test):\n",
    "    \n",
    "    inputs_columns = nombre_series_horaria\n",
    "\n",
    "    len_input_features = len(inputs_columns)\n",
    "    len_output_features = len(output_columns)\n",
    "\n",
    "    results = Ep.SplitTimeseriesMultipleTimesBackAhead(df=data,\n",
    "                                                       day=day,\n",
    "                                                       start_date_train=start_date_train,\n",
    "                                                       start_date_val=start_date_val,\n",
    "                                                       start_date_test=start_date_test,\n",
    "                                                       end_date_test=end_date_test,\n",
    "                                                       n_steps_out=n_steps_out,\n",
    "                                                       n_steps_in=n_steps_in,\n",
    "                                                       overlap=overlap,\n",
    "                                                       input_features=inputs_columns,\n",
    "                                                       output_features=output_columns)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_callbacks():\n",
    "    \n",
    "    callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                              factor=0.1,\n",
    "                                                              min_lr=1e-5,\n",
    "                                                              patience=5,\n",
    "                                                              verbose=1)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=10,\n",
    "                                                      mode='min')\n",
    "\n",
    "    callbacks = [callback_reduce_lr,early_stopping]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_arquitecturas(input_shape,n_steps_out):\n",
    "\n",
    "    arquitectura31 = CustomHyperModel.Arquitectura31(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura32 = CustomHyperModel.Arquitectura32(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura36 = CustomHyperModel.Arquitectura36(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura37 = CustomHyperModel.Arquitectura37(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura43 = CustomHyperModel.Arquitectura43(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura45 = CustomHyperModel.Arquitectura45(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "    arquitectura46 = CustomHyperModel.Arquitectura46(input_shape=input_shape,n_steps_out=n_steps_out)\n",
    "\n",
    "    arq_list = [arquitectura31,arquitectura32,arquitectura36,arquitectura37,arquitectura43,arquitectura45,arquitectura46]\n",
    "    \n",
    "    return arq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_path = os.path.join('..','..','dataset','Series','Sabanas','Original','Sabana_Datos_Horaria.xlsx')\n",
    "data_horaria = pd.read_excel(data_horaria_path)\n",
    "data_horaria = data_horaria.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_series_horaria = data_horaria.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_bolsa_path = os.path.join('..','..','dataset','Series','Sabanas','Original','Sabana_Datos_Precio_Bolsa.xlsx')\n",
    "precio_bolsa = pd.read_excel(precio_bolsa_path)\n",
    "precio_bolsa = precio_bolsa.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_full = pd.concat([data_horaria,precio_bolsa],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_full['day_of_week'] = data_horaria_full.index.day_name()\n",
    "precio_bolsa['day_of_week'] = precio_bolsa.index.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = np.array(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "\n",
    "start_date_train = '2000-02-01'\n",
    "start_date_val = '2020-01-01'\n",
    "start_date_test = '2020-04-01'\n",
    "end_date_test = '2020-05-01'\n",
    "n_steps_out=24\n",
    "output_columns = ['$kWh']\n",
    "len_output_features = len(output_columns)\n",
    "n_steps_in = 48\n",
    "overlap = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 05s]\n",
      "val_loss: 3694.46240234375\n",
      "\n",
      "Best val_loss So Far: 731.3621826171875\n",
      "Total elapsed time: 00h 02m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "dict_days_test = dict()\n",
    "\n",
    "for j,d in enumerate(Days):\n",
    "    \n",
    "    results_from_hourly = get_dataset_hourly(n_steps_in=n_steps_in,n_steps_out=n_steps_out,overlap=overlap,\n",
    "                                             nombre_series_horaria=nombre_series_horaria,\n",
    "                                             output_columns=output_columns,data=data_horaria_full,\n",
    "                                             day=d,start_date_train=start_date_train,\n",
    "                                             start_date_val=start_date_val,start_date_test=start_date_test,\n",
    "                                             end_date_test=end_date_test)\n",
    "    \n",
    "    trainX_H,trainY_H,valX_H,valY_H,testX_H,testY_H,scaler_H_x,scaler_H_y,df2_H,dataset_H = results_from_hourly\n",
    "    \n",
    "    hourly_input_shape = (trainX_H.shape[1],trainX_H.shape[2])\n",
    "    callbacks = crear_callbacks()\n",
    "    arq_list = generar_arquitecturas(hourly_input_shape,n_steps_out)\n",
    "    \n",
    "    arq_names = [31,32,36,37,43,45,46]\n",
    "    arq_idx = 0\n",
    "    arq_best_models = dict()\n",
    "\n",
    "    for arq in arq_list:\n",
    "        \n",
    "        project_name = '{}-Arquitectura{}'.format(d,arq_names[arq_idx])\n",
    "\n",
    "        bayesian_tuner = BayesianOptimization(\n",
    "            arq,\n",
    "            objective='val_loss',\n",
    "            num_initial_points=1,\n",
    "            max_trials=10,\n",
    "            directory=os.path.normpath('C:/my_dir'),\n",
    "            project_name=project_name\n",
    "        )\n",
    "\n",
    "        # Overview of the task\n",
    "        bayesian_tuner.search_space_summary()\n",
    "\n",
    "        # Performs the hyperparameter tuning\n",
    "        search_start = time.time()\n",
    "        bayesian_tuner.search(x=trainX_H,y=trainY_H,\n",
    "                          epochs=200,\n",
    "                          validation_data=(valX_H,valY_H),\n",
    "                          callbacks=callbacks)\n",
    "        search_end = time.time()\n",
    "        elapsed_time = search_end - search_start\n",
    "\n",
    "        dict_key = project_name\n",
    "\n",
    "        arq_best_models[dict_key] = dict()\n",
    "        bs_model = bayesian_tuner.oracle.get_best_trials(1)[0]\n",
    "\n",
    "        model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "        trainPredict,trainY_true,testPredict,testY_true = make_predictions(model,scaler_H_y,trainX_H,trainY_H,valX_H,valY_H,\n",
    "                                                                           n_steps_out,len_output_features)\n",
    "\n",
    "        trainMAPE,testMAPE,train_sMAPE,test_sMAPE = get_metrics(trainY_true,trainPredict,testY_true,testPredict)\n",
    "\n",
    "        arq_best_models[dict_key]['Score'] = bs_model.score\n",
    "        arq_best_models[dict_key]['Tiempo Scaneo'] = elapsed_time\n",
    "        arq_best_models[dict_key]['Mape Train'] = trainMAPE\n",
    "        arq_best_models[dict_key]['Mape Test'] = testMAPE\n",
    "        arq_best_models[dict_key]['sMape Train'] = train_sMAPE\n",
    "        arq_best_models[dict_key]['sMape Test'] = test_sMAPE\n",
    "\n",
    "        if bs_model.hyperparameters.values:\n",
    "            for hp, value in bs_model.hyperparameters.values.items():\n",
    "                arq_best_models[dict_key][hp] = value\n",
    "\n",
    "        arq_idx += 1\n",
    "        \n",
    "    with open('{}-BestModels.json'.format(d), 'w') as outfile:\n",
    "        json.dump(arq_best_models, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proyecto Grados",
   "language": "python",
   "name": "proyecto-grados"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
