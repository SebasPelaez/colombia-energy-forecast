{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar Dependencias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que no se tengan instaladas estos paquetes, recomiendo instalarlos en las versiones a continuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3==1.16.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow==7.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "sys.path.append('..')\n",
    "import CustomHyperModelImages\n",
    "import EnergyPricesLibrary as Ep\n",
    "\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,scaler_y,trainX,trainY,testX,testY,n_steps_out,len_output_features):\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    trainPredict = trainPredict.reshape(trainPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    testPredict  = model.predict(testX)\n",
    "    testPredict  = testPredict.reshape(testPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY_ = scaler_y.inverse_transform(trainY.reshape(trainY.shape[0]*n_steps_out,len_output_features))\n",
    "    \n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY_ = scaler_y.inverse_transform(testY.reshape(testY.shape[0]*n_steps_out,len_output_features))\n",
    "        \n",
    "    return trainPredict,trainY_,testPredict,testY_\n",
    "\n",
    "def get_metrics(trainY,trainPredict,testY,testPredict):\n",
    "    \n",
    "    trainMAPE  = Ep.MAPE(trainPredict,trainY)\n",
    "    testMAPE  = Ep.MAPE(testPredict,testY)\n",
    "    \n",
    "    train_sMAPE  = Ep.sMAPE(trainY,trainPredict)\n",
    "    test_sMAPE  = Ep.sMAPE(testY,testPredict)\n",
    "    \n",
    "    return trainMAPE,testMAPE,train_sMAPE,test_sMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('dataset'):\n",
    "    s3_resource = boto3.resource('s3',\n",
    "                                 aws_access_key_id='AKIA4NVVYWBFHY2KRSMC',\n",
    "                                 aws_secret_access_key='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')\n",
    "    bucket = s3_resource.Bucket('colombia-energy-forecast')\n",
    "\n",
    "    for obj in bucket.objects.filter():\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        if '.xlsx' in obj.key or '.jpg' in obj.key:\n",
    "            bucket.download_file(obj.key, obj.key) # save to same path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatic_images_prcp_dir = os.path.join('dataset','Climatic Images','PRCP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatic_images_tavg_dir = os.path.join('dataset','Climatic Images','TAVG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_bolsa_path = os.path.join('dataset','Series','Sabanas','Original','Sabana_Datos_Precio_Bolsa.xlsx')\n",
    "precio_bolsa = pd.read_excel(precio_bolsa_path)\n",
    "precio_bolsa = precio_bolsa.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_fechas = list()\n",
    "lista_rutas = list()\n",
    "for prcp_file,tavg_file in zip(os.listdir(climatic_images_prcp_dir),os.listdir(climatic_images_tavg_dir)):\n",
    "    fecha = prcp_file.split('.')[0]\n",
    "    ruta_prcp = os.path.join(climatic_images_prcp_dir,prcp_file)\n",
    "    ruta_tavg = os.path.join(climatic_images_tavg_dir,tavg_file)\n",
    "    lista_fechas.append(fecha)\n",
    "    lista_rutas.append([ruta_prcp,ruta_tavg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 'All'\n",
    "start_date_train = '2000-02-01'\n",
    "start_date_val = '2020-01-01'\n",
    "start_date_test = '2020-04-01'\n",
    "end_date_test = '2020-05-01'\n",
    "n_steps_out=24\n",
    "output_columns = ['$kWh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(lista_rutas,index=lista_fechas,columns=['Precipitacion','Temperatura'])\n",
    " \n",
    "n_steps_in  = 15\n",
    "overlap = 1\n",
    "len_output_features = len(output_columns)\n",
    "\n",
    "IMG_HEIGHT,IMG_WIDTH = 128,128\n",
    "\n",
    "results = Ep.SplitTimeseriesMultipleTimesBackAhead_DifferentTimes_Images(df_x=dataset_df,df_y=precio_bolsa,\n",
    "                                                                         start_date_train=start_date_train,\n",
    "                                                                         start_date_val=start_date_val,\n",
    "                                                                         start_date_test=start_date_test,\n",
    "                                                                         end_date_test=end_date_test,n_steps_out=n_steps_out,\n",
    "                                                                         n_steps_in=n_steps_in,overlap=overlap,\n",
    "                                                                         output_features=output_columns,\n",
    "                                                                         IMG_HEIGHT=IMG_HEIGHT,IMG_WIDTH=IMG_WIDTH)\n",
    "\n",
    "trainX_I,trainY_I,valX_I,valY_I,testX_I,testY_I,scaler_y_I,dataset_x_I,dataset_y_I = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Train:',trainX_I.shape, trainY_I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Val:',valX_I.shape,valY_I.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Test:',testX_I.shape, testY_I.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                          factor=0.1,\n",
    "                                                          min_lr=1e-5,\n",
    "                                                          patience=5,\n",
    "                                                          verbose=1)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                  patience=10,\n",
    "                                                  mode='min')\n",
    "\n",
    "callbacks = [callback_reduce_lr,early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = trainX_I[0].shape\n",
    "\n",
    "arquitectura1 = CustomHyperModelImages.ArquitecturaI1(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura2 = CustomHyperModelImages.ArquitecturaI2(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura3 = CustomHyperModelImages.ArquitecturaI3(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura4 = CustomHyperModelImages.ArquitecturaI4(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura5 = CustomHyperModelImages.ArquitecturaI5(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura6 = CustomHyperModelImages.ArquitecturaI6(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura7 = CustomHyperModelImages.ArquitecturaI7(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura8 = CustomHyperModelImages.ArquitecturaI8(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura9 = CustomHyperModelImages.ArquitecturaI9(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura10 = CustomHyperModelImages.ArquitecturaI10(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura11 = CustomHyperModelImages.ArquitecturaI11(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)\n",
    "arquitectura12 = CustomHyperModelImages.ArquitecturaI12(input_shape=INPUT_SHAPE,n_steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_list = [arquitectura1,arquitectura2,arquitectura3,arquitectura4,\n",
    "            arquitectura5,arquitectura6,arquitectura7,arquitectura8,\n",
    "            arquitectura9,arquitectura10,arquitectura11,arquitectura12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_idx = 1\n",
    "arq_best_models = dict()\n",
    "\n",
    "for arq in arq_list:\n",
    "    \n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        arq,\n",
    "        objective='val_mean_absolute_percentage_error',\n",
    "        num_initial_points=1,\n",
    "        max_trials=10,\n",
    "        directory='dir_images_15',\n",
    "        project_name=str(arq_idx)\n",
    "    )\n",
    "    \n",
    "    # Overview of the task\n",
    "    bayesian_tuner.search_space_summary()\n",
    "    \n",
    "    # Performs the hyperparameter tuning\n",
    "    search_start = time.time()\n",
    "    bayesian_tuner.search(x=trainX_I,y=trainY_I,\n",
    "                      epochs=200,\n",
    "                      validation_data=(valX_I,valY_I),\n",
    "                      callbacks=callbacks)\n",
    "    search_end = time.time()\n",
    "    elapsed_time = search_end - search_start\n",
    "    \n",
    "    print('Tiempo Total Transcurrido {}'.format(elapsed_time))\n",
    "    \n",
    "    dict_key = 'Arquitectura {}'.format(arq_idx)\n",
    "\n",
    "    arq_best_models[dict_key] = dict()\n",
    "    bs_model = bayesian_tuner.oracle.get_best_trials(1)[0]\n",
    "    \n",
    "    model = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    trainPredict,trainY_true,testPredict,testY_true = make_predictions(model,scaler_y_I,trainX_I,trainY_I,valX_I,valY_I,\n",
    "                                                             n_steps_out,len_output_features)\n",
    "    \n",
    "    trainMAPE,testMAPE,train_sMAPE,test_sMAPE = get_metrics(trainY_true,trainPredict,testY_true,testPredict)\n",
    "\n",
    "    arq_best_models[dict_key]['Score'] = bs_model.score\n",
    "    arq_best_models[dict_key]['Tiempo Scaneo'] = elapsed_time\n",
    "    arq_best_models[dict_key]['Mape Train'] = trainMAPE\n",
    "    arq_best_models[dict_key]['Mape Test'] = testMAPE\n",
    "\n",
    "    if bs_model.hyperparameters.values:\n",
    "        for hp, value in bs_model.hyperparameters.values.items():\n",
    "            arq_best_models[dict_key][hp] = value\n",
    "    \n",
    "    arq_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BestModels-Images-I15.json', 'w') as outfile:\n",
    "    json.dump(arq_best_models, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq_best_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proyecto Grados",
   "language": "python",
   "name": "proyecto-grados"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
