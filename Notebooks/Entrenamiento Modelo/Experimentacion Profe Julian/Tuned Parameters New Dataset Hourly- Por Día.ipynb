{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import CustomMetrics\n",
    "import EnergyPricesLibrary as Ep\n",
    "\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,scaler_y,trainX,trainY,testX,testY,n_steps_out,len_output_features):\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    trainPredict = trainPredict.reshape(trainPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    testPredict  = model.predict(testX)\n",
    "    testPredict  = testPredict.reshape(testPredict.shape[0]*n_steps_out,len_output_features)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler_y.inverse_transform(trainPredict)\n",
    "    trainY_ = scaler_y.inverse_transform(trainY.reshape(trainY.shape[0]*n_steps_out,len_output_features))\n",
    "    \n",
    "    testPredict = scaler_y.inverse_transform(testPredict)\n",
    "    testY_ = scaler_y.inverse_transform(testY.reshape(testY.shape[0]*n_steps_out,len_output_features))\n",
    "        \n",
    "    return trainPredict,trainY_,testPredict,testY_\n",
    "\n",
    "def get_metrics(trainY,trainPredict,testY,testPredict):\n",
    "    \n",
    "    trainMAPE  = Ep.MAPE(trainPredict,trainY)\n",
    "    testMAPE  = Ep.MAPE(testPredict,testY)\n",
    "    \n",
    "    train_sMAPE  = Ep.sMAPE(trainY,trainPredict)\n",
    "    test_sMAPE  = Ep.sMAPE(testY,testPredict)\n",
    "    \n",
    "    return trainMAPE,testMAPE,train_sMAPE,test_sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_hourly(n_steps_in,n_steps_out,overlap,nombre_series_horaria,output_columns,data,day,\n",
    "                       start_date_train,start_date_val,start_date_test,end_date_test):\n",
    "    \n",
    "    inputs_columns = nombre_series_horaria\n",
    "\n",
    "    len_input_features = len(inputs_columns)\n",
    "    len_output_features = len(output_columns)\n",
    "\n",
    "    results = Ep.SplitTimeseriesMultipleTimesBackAhead(df=data,\n",
    "                                                       day=day,\n",
    "                                                       start_date_train=start_date_train,\n",
    "                                                       start_date_val=start_date_val,\n",
    "                                                       start_date_test=start_date_test,\n",
    "                                                       end_date_test=end_date_test,\n",
    "                                                       n_steps_out=n_steps_out,\n",
    "                                                       n_steps_in=n_steps_in,\n",
    "                                                       overlap=overlap,\n",
    "                                                       input_features=inputs_columns,\n",
    "                                                       output_features=output_columns)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_callbacks():\n",
    "    \n",
    "    callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                              factor=0.1,\n",
    "                                                              min_lr=1e-5,\n",
    "                                                              patience=5,\n",
    "                                                              verbose=1)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=10,\n",
    "                                                      mode='min')\n",
    "\n",
    "    callbacks = [callback_reduce_lr,early_stopping]\n",
    "    \n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_I2(hourly_input_shape,n_steps_out=24):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.LSTM(\n",
    "            input_shape=hourly_input_shape,\n",
    "            units=320,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.0975),\n",
    "            dropout=0.27,\n",
    "            return_sequences=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.LSTM(   \n",
    "            units=448,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.03),\n",
    "            dropout=0.36,\n",
    "            return_sequences=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=n_steps_out,activation=None))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(0.0002),\n",
    "        loss=CustomMetrics.symmetric_mean_absolute_percentage_error,\n",
    "        metrics=[tf.metrics.MeanAbsoluteError(),\n",
    "                 tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                 CustomMetrics.symmetric_mean_absolute_percentage_error]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_I3(hourly_input_shape,n_steps_out=24):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.LSTM(\n",
    "            input_shape=hourly_input_shape,\n",
    "            units=128,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.06),\n",
    "            dropout=0.09,\n",
    "            return_sequences=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.LSTM(   \n",
    "            units=384,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.0225),\n",
    "            dropout=0,\n",
    "            return_sequences=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=n_steps_out,activation=None))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(0.000103025),\n",
    "        loss=CustomMetrics.symmetric_mean_absolute_percentage_error,\n",
    "        metrics=[tf.metrics.MeanAbsoluteError(),\n",
    "                 tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                 CustomMetrics.symmetric_mean_absolute_percentage_error]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_I5(hourly_input_shape,n_steps_out=24):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.GRU(\n",
    "            input_shape=hourly_input_shape,\n",
    "            units=448,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.06),\n",
    "            dropout=0.36,\n",
    "            return_sequences=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        tf.keras.layers.GRU(   \n",
    "            units=128,\n",
    "            activation='tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(l1=0.09),\n",
    "            dropout=0.36,\n",
    "            return_sequences=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=n_steps_out,activation=None))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(0.00027),\n",
    "        loss=CustomMetrics.symmetric_mean_absolute_percentage_error,\n",
    "        metrics=[tf.metrics.MeanAbsoluteError(),\n",
    "                 tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "                 CustomMetrics.symmetric_mean_absolute_percentage_error]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_path = os.path.join('dataset','Series','Sabanas','Original','Sabana_Datos_Horaria.xlsx')\n",
    "data_horaria = pd.read_excel(data_horaria_path)\n",
    "data_horaria = data_horaria.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_series_horaria = data_horaria.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_bolsa_path = os.path.join('dataset','Series','Sabanas','Original','Sabana_Datos_Precio_Bolsa.xlsx')\n",
    "precio_bolsa = pd.read_excel(precio_bolsa_path)\n",
    "precio_bolsa = precio_bolsa.set_index('Fecha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_full = pd.concat([data_horaria,precio_bolsa],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_horaria_full['day_of_week'] = data_horaria_full.index.day_name()\n",
    "precio_bolsa['day_of_week'] = precio_bolsa.index.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = np.array(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "\n",
    "start_date_train = '2000-02-01'\n",
    "start_date_val = '2020-01-01'\n",
    "start_date_test = '2020-04-01'\n",
    "end_date_test = '2020-05-01'\n",
    "n_steps_out=24\n",
    "output_columns = ['$kWh']\n",
    "len_output_features = len(output_columns)\n",
    "n_steps_in = 120\n",
    "overlap = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_days_test = dict()\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, sharex='all',figsize=(10,7))\n",
    "fig.set_size_inches(20,20)\n",
    "k1 = 0\n",
    "k2 = 0\n",
    "\n",
    "for j,d in enumerate(Days):\n",
    "    \n",
    "    results_from_hourly = get_dataset_hourly(n_steps_in=n_steps_in,n_steps_out=n_steps_out,overlap=overlap,\n",
    "                                             nombre_series_horaria=nombre_series_horaria,\n",
    "                                             output_columns=output_columns,data=data_horaria_full,\n",
    "                                             day=d,start_date_train=start_date_train,\n",
    "                                             start_date_val=start_date_val,start_date_test=start_date_test,\n",
    "                                             end_date_test=end_date_test)\n",
    "    \n",
    "    trainX_H,trainY_H,valX_H,valY_H,testX_H,testY_H,scaler_H_x,scaler_H_y,dataset_x,dataset_y = results_from_hourly\n",
    "    \n",
    "    trainX_H = np.concatenate([trainX_H,valX_H])\n",
    "    trainY_H = np.concatenate([trainY_H,valY_H])\n",
    "    \n",
    "    hourly_input_shape = (trainX_H.shape[1],trainX_H.shape[2])\n",
    "    callbacks = crear_callbacks()\n",
    "    model = build_model_I5(hourly_input_shape,n_steps_out)\n",
    "    \n",
    "    model.fit(trainX_H, trainY_H, validation_data=(testX_H,testY_H),epochs=200,callbacks=callbacks,verbose=1)\n",
    "    \n",
    "    trainPredict,trainY_true,testPredict,testY_true = make_predictions(model,scaler_H_y,trainX_H,trainY_H,testX_H,testY_H,\n",
    "                                                                       n_steps_out,len_output_features)\n",
    "\n",
    "    trainMAPE,testMAPE,train_sMAPE,test_sMAPE = get_metrics(trainY_true,trainPredict,testY_true,testPredict)\n",
    "\n",
    "    dict_days_test[d] = dict()\n",
    "    dict_days_test[d]['trainPredict'] = trainPredict\n",
    "    dict_days_test[d]['testPredict'] = testPredict\n",
    "    dict_days_test[d]['testY'] = testY_true\n",
    "    dict_days_test[d]['trainMAPE'] = trainMAPE\n",
    "    dict_days_test[d]['testMAPE'] = testMAPE\n",
    "    dict_days_test[d]['train_sMAPE'] = train_sMAPE\n",
    "    dict_days_test[d]['test_sMAPE'] = test_sMAPE\n",
    "    \n",
    "    Nt = trainPredict.shape[0] + testPredict.shape[0]\n",
    "    trainPredictPlot = np.zeros((Nt,1))\n",
    "    trainPredictPlot[:,:] = np.nan\n",
    "    trainPredictPlot[:len(trainPredict), :] = np.concatenate((dataset_y[0].reshape(1,1),trainPredict[:-1]))\n",
    "\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.zeros((Nt,1))\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(trainPredict):Nt, :] = testPredict\n",
    "    \n",
    "    k1 = j\n",
    "    if j > 3:\n",
    "        k1 = j-4\n",
    "        k2 = 1\n",
    "    \n",
    "    axs[k1,k2].plot(np.concatenate((trainY_true,testY_true)),label='Original data')\n",
    "    axs[k1,k2].plot(trainPredictPlot,label='Training predictions')\n",
    "    axs[k1,k2].plot(testPredictPlot,label='Test prediction')\n",
    "    \n",
    "    axs[k1,k2].set_title(d)\n",
    "    axs[k1,k2].legend()\n",
    "    axs[k1,k2].set_ylabel('COL/kWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Days = np.array(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "fig, axs = plt.subplots(4, 2, sharex='all',figsize=(15,8))\n",
    "fig.set_size_inches(20,20)\n",
    "k1 = 0\n",
    "k2 = 0\n",
    "\n",
    "for j,d in enumerate(Days):\n",
    "    \n",
    "    trainPredict = dict_days_test[d]['trainPredict']\n",
    "    testPredict = dict_days_test[d]['testPredict']\n",
    "    testY = dict_days_test[d]['testY']\n",
    "    \n",
    "    trainMAPE = dict_days_test[d]['trainMAPE']\n",
    "    testMAPE = dict_days_test[d]['testMAPE']\n",
    "    train_sMAPE = dict_days_test[d]['train_sMAPE']\n",
    "    test_sMAPE = dict_days_test[d]['test_sMAPE']\n",
    "    \n",
    "    Nt = trainPredict.shape[0] + testPredict.shape[0]\n",
    "    \n",
    "    print('{} -> Train Mape:{},Test Mape:{},Train sMAPE:{}, Test sMAPE:{}'.format(d,trainMAPE,testMAPE,train_sMAPE,test_sMAPE))    \n",
    "    \n",
    "    testOriginalPlot = np.zeros((Nt,1))\n",
    "    testOriginalPlot[:, :] = np.nan\n",
    "    testOriginalPlot[len(trainPredict):Nt, :] = testY\n",
    "    \n",
    "    testPredictPlot = np.zeros((Nt,1))\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(trainPredict):Nt, :] = testPredict\n",
    "    \n",
    "    k1 = j\n",
    "    if j > 3:\n",
    "        k1 = j-4\n",
    "        k2 = 1\n",
    "    \n",
    "    axs[k1,k2].plot(testOriginalPlot,label='Original data')\n",
    "    axs[k1,k2].plot(testPredictPlot,label='Test prediction')\n",
    "    \n",
    "    axs[k1,k2].set_title(d)\n",
    "    axs[k1,k2].legend()\n",
    "    axs[k1,k2].set_ylabel('COL/kWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proyecto Grados",
   "language": "python",
   "name": "proyecto-grados"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
